<html>
    <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
      <!--
      <script src="./resources/jsapi" type="text/javascript"></script>
      <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
     -->
    
    <style type="text/css">
      body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
        text-align: justify;
      }
      h1 {
        font-weight:300;
      }
      h2 {
        font-weight:300;
      }
      .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
      }
      video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      a:link,a:visited
      {
        color: #1367a7;
        text-decoration: none;
      }
      a:hover {
        color: #208799;
      }
      td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
      }

      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
      }
      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }
      .vert-cent {
        position: relative;
          top: 50%;
          transform: translateY(-50%);
      }
      hr
      {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
      }
      #imageContainer {
        display: none;
      }
      #toggleButton {
        background: none;
        border: none;
        font-weight: bold;
        font-size: 26pt;
        cursor: pointer;
        text-decoration: underline;
        color: darkorchid;
      }
      #toggleButton.clicked {
        color: darkgray; /* Change this to the desired color */
      }
    </style>

    <script>
      function toggleImage() {
        var imageContainer = document.getElementById("imageContainer");
        var button = document.getElementById("toggleButton")

        if (imageContainer.style.display === "block") {
          button.classList.remove("clicked");      
          imageContainer.style.display = "none";
          button.innerHTML = "&#x25BC;Click here for more results..."
        } else {
          button.classList.add("clicked");
          imageContainer.style.display = "block";
          button.innerHTML = "&#x25B2;Click here to hide results..."
        }
      }
    </script>
    
    
    
        <title>Data Attribution for Text-to-Image Models by Unlearning Synthesized Images</title>
        <meta property="og:image" content="http://peterwang512.github.io/AttributeByUnlearning/files/teaser.jpg">
        <meta property="og:title" content="Data Attribution for Text-to-Image Models by Unlearning Synthesized Images">
      </head>
    
      <body>
            <br>
              <center>
                <span style="font-size:32px">Data Attribution for Text-to-Image Models by Unlearning Synthesized Images</span><br><br>
    
              <table align="center" width="850px">
                <tbody><tr>
                        <td align="center" width="205px">
                  <center>
                    <span style="font-size:20px"><a href="http://peterwang512.github.io">Sheng-Yu Wang</a><sup>1</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="https://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a><sup>2</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a><sup>3</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="http://cs.cmu.edu/~junyanz">Jun-Yan Zhu</a><sup>1</sup></span>
                    </center>
                    </td>
                        <td align="center" width="175px">
                  <center>
                    <span style="font-size:20px"><a href="http://richzhang.github.io/">Richard Zhang</a><sup>2</sup></span>
                    </center>
                    </td>
                </tr>
            </tbody></table>
    
              <table align="center" width="700px">
                <tbody><tr>
                        <td align="center" width="100px">
                  <center>
                        <span style="font-size:20px"></span>
                    </center>
                    </td>
                        <td align="center" width="400px">
                  <center>
                        <span style="font-size:20px"><sup>1</sup>Carnegie Mellon University</span>
                    </center>
                    </td>
                        <td align="center" width="250px">
                  <center>
                        <span style="font-size:20px"><sup>2</sup>Adobe Research</span>
                    </center>
                    </td>
                        <td align="center" width="250px">
                  <center>
                        <span style="font-size:20px"><sup>3</sup>UC Berkeley</span>
                    </center>
                    </td>
                        <td align="center" width="100px">
                  <center>
                        <span style="font-size:20px"></span>
                    </center>
                    </td>
            </tr></tbody></table>

    
              <table align="center" width="500px">
                <tbody><tr>
    <!--                     <td align="center" width="50px">
                  <center>
                        <span style="font-size:18px"></span>
                    </center>
                    </td> -->
                        <td align="center" width="160px">
                  <center>
                    <br>
                    <span style="font-size:20px"><a href="https://github.com/peterwang512/AttributeByUnlearning"> [Code]</a></span>
                    </center>
                    </td>
                        <td align="center" width="160px">
                  <center>
                    <br>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2406.09408"> <!-- [Paper] -->[Paper]</a></span>
                    </center>
                    </td>
<!--                      <td align="center" width="160px">
                  <center>
                    <br>
                    <span style="font-size:20px"><a href="https://www.cs.cmu.edu/~dataattribution/files/attribution_slides.pptx"> [Slides]</a></span>
                    </center>
                    </td>
                     <td align="center" width="160px">
                  <center>
                    <br>
                    <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=pUx_rvTD5Rw&ab_channel=RichardZhang">[Talk]</a></span>
                    </center>
                    </td>
 -->
    
            </tr></tbody></table>
                                        <!-- <p> In ArXiv, 2023. </p> -->
              </center>
            <br>
            <table align="center" width="1000px">
              <tbody><tr>
                      <td width="400px">
                <center>
<!--                     <video id="teaser_video" width="1024px" loop src="./files/short_vid_v2.mp4" autoplay muted controls>
                        Your browser does not support HTML5 Player 
                    </video> -->
                    <img src="files/teaser.jpg" width="960px">
                </center>
                      </td>
                      </tr>
                      </tbody></table>
          <hr>
    
            <center><h2>Abstract</h2></center>
The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. We can define "influence" by saying that, for a given output, if a model is retrained from scratch without that output's most influential images, the model should then fail to generate that output image. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining from scratch. We propose a new approach that efficiently identifies highly-influential images. Specifically, we simulate unlearning the synthesized image, proposing a method to increase the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. Then, we find training images that are forgotten by proxy, identifying ones with significant loss deviations after the unlearning process, and label these as influential. We evaluate our method with a computationally intensive but "gold-standard" retraining from scratch and demonstrate our method's advantages over previous methods.
    
    
    
    
    
<!--           <br><hr>

        <center><h2>Method</h2></center><table align="center" width="700" px=""> -->

        <br><hr>
    
          <center><h2>Results</h2></center>
    
    
           <p><b>Attribution results on MSCOCO models.</b>We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers. Next, we proceed with the counterfactual analysis, where we test whether these attributed images are "truly influential". </p>
           
           <center>
            <img src="./files/coco_attr.jpg" width="1100px">
            </center>
    

           <p><b>Counterfactual evaluation by removing influential images.</b> We compare images across our method and baselines generated by leave-K-out models, using different K values, all under the same random noise and text prompt. A significant deviation in regeneration indicates that critical, influential images were identified by the attribution algorithm. While baselines regenerate similar images to the original, our method generates ones that deviate significantly, even with as few as 500 influential images removed (∼0.42% of the dataset).  </p>
           <center>
            <img src="./files/coco_leave_k_out.jpg" width="1100px">
            </center>


    

        <p><b>Spatially-localized attribution.</b> Given a synthesized image (left), we crop regions containing specific objects using <a href="https://github.com/IDEA-Research/GroundingDINO">GroundingDINO</a>. We attribute each object separately by only running forgetting on the pixels within the cropped region. Our method can attribute different synthesized regions to different training images. </p>
        <center>
            <img src="./files/coco_composition.jpg" width="1100px">
            </center>

            <hr>
        <p><b>Evaluating on <a href="https://peterwang512.github.io/GenDataAttribution/">Customized Model Benchmark</a>.</b> We evaluate on this benchmark for attribution large-scale text-to-image models that focuses on a specialized form of attribution: attributing customized models trained on an individual or a few exemplar images. The red boxes indicate ground truth exemplar images used for customizing the model. DINO (AbC) and CLIP (AbC) correspond to DINO and CLIP features finetuned directly on the benchmark, respectively. Both our method and baselines successfully identify the exemplar images on object-centric models (left), while our method outperforms the baselines with artistic style models (right) </p>
        <center>
            <img src="./files/abc_attr.jpg" width="1100px">
            </center>

            <hr>
            <center><h2>Paper</h2></center><table align="center" width="700" px="">
    
              <tbody><tr>
              <td><a href="https://arxiv.org/abs/2406.09408"><img class="layered-paper-big" style="height:175px" src="./files/firstpg.jpg"></a></td>
              <td><span style="font-size:12pt">Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang.</span><br>
              <b><span style="font-size:12pt">Data Attribution for Text-to-Image Models by Unlearning Synthesized Images.</span></b><br>
              <span style="font-size:12pt">In ArXiv, 2024. (<a href="https://arxiv.org/abs/2406.09408">Paper</a>)</span>
              </td>
    
              <br>
              <table align="center" width="600px">
                <tbody>
                  <tr>
                    <td>
                      <center>
                        <span style="font-size:22px">
                          <a href="./files/bibtex.txt" target="_blank">[Bibtex]</a>
                        </span>
                      </center>
                    </td>
                  </tr>
                </tbody>
              </table>
          <br>


<!--     <br><hr>
    <table align="center" width="1000px">
      <tbody><tr>
              <td width="400px">
        <left>
      <center><h2>Related works</h2></center>
    </left>
    <p><b>Data attribution for classifiers:</b></p>
    <ul>
      <li>Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. <a href="https://arxiv.org/abs/2303.14186">"TRAK: Attributing Model Behavior at Scale."</a>. In ArXiv 2023.</li><br>
      <li>Vitaly Feldman and Chiyuan Zhang. <a href="https://arxiv.org/abs/2008.03703">"What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation."</a>. In NeurIPS 2020.</li><br>
      <li>Pang Wei Koh and Percy Liang. <a href="https://arxiv.org/abs/1703.04730">"Understanding Black-box Predictions via Influence Functions."</a>. In ICML 2017.</li><br>
    </ul>

    </td>
    </tr>
    </tbody></table> -->



    
    
          <br><hr>
    
            <table align="center" width="1100px">
              <tbody><tr>
                      <td width="400px">
                <left>
              <center><h2>Acknowledgements</h2></center>
              We thank Kristian Georgiev for answering all of our inquiries regarding JourneyTRAK implementation and evaluation, and providing us their models and an earlier version of JourneyTRAK code. We thank Nupur Kumari, Kangle Deng, Grace Su for feedback on the draft. This work is partly supported by the Packard Fellowship, JPMC Faculty Research Award, and NSF IIS-2239076. Website template is from <a href="https://richzhang.github.io/colorization/">Colorful Colorization</a>.
          </left>
        </td>
           </tr>
        </tbody></table>
    
        <br>

        <hr>
        <center><h2>Citation</h2></center>

            <code style="display:block; background:#D3D3D3;">
                @article{wang2024attributebyunlearning,<br>
                &nbsp; title={Data Attribution for Text-to-Image Models by Unlearning Synthesized Images},<br>
                &nbsp; author={Wang, Sheng-Yu and Hertzmann, Aaron and Efros, Alexei A and Zhu, Jun-Yan and Zhang, Richard},<br>
                &nbsp; journal={arXiv preprint arXiv:2406.09408},<br>
                &nbsp; year = {2024},<br>
                &nbsp; }</code>
        <br>


	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-L56W4JWSEP"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-L56W4JWSEP');
	</script>
    
    </body></html>
